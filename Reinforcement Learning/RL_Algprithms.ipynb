{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8b4309",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Algorithms\n",
    "### Author: Beixian Gu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c7be7",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "- Introduction\n",
    "    - Tabular RL\n",
    "    - Approximate RL\n",
    "- Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c76771",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Reinforcement Learning (RL) is a branch of machine learning that deals with how an agent can learn to make decisions in a dynamic environment, by interacting with it and receiving feedback in the form of rewards or punishments. There are two main types of RL algorithms: Tabular RL and Approximate RL.\n",
    "\n",
    "## Tabular RL\n",
    "\n",
    "Tabular RL is a type of RL algorithm that works by constructing a table of values that represent the expected rewards of taking different actions in different states of the environment. The agent uses this table to make decisions based on the highest expected reward.\n",
    "\n",
    "### The k-armed bandit\n",
    "\n",
    "The k-armed bandit is a simple example of a Tabular RL problem, where an agent has to choose between k different actions (or arms) to maximize its reward. The agent starts with no knowledge of the rewards associated with each action, and it has to learn by trial and error which action to take.\n",
    "\n",
    "### Action-value methods\n",
    "\n",
    "Action-value methods are a family of Tabular RL algorithms that estimate the value of each action in each state by updating the Q-value of that action using the Bellman equation. The most popular action-value method is the Q-learning algorithm.\n",
    "\n",
    "$$\n",
    "Q(s,a) = Q(s,a) + \\alpha(r + \\gamma max(Q(s',a')) - Q(s,a))\n",
    "$$\n",
    "where $Q(s,a)$ is the estimated value of taking action a in state $s$, $r$ is the reward received after taking action a in state $s$, $s'$ is the new state reached after taking action a in state $s$, $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor.\n",
    "\n",
    "### Making a Q-table\n",
    "\n",
    "To implement a Tabular RL algorithm, the agent needs to construct a Q-table that stores the expected rewards of each action in each state. The Q-table is initialized with random values, and it is updated after each interaction with the environment.\n",
    "\n",
    "## Approximate RL\n",
    "\n",
    "Approximate RL is a type of RL algorithm that works by approximating the value function using a function approximator, such as a neural network. The agent uses this function to make decisions based on the highest expected reward.\n",
    "\n",
    "### Function approximation\n",
    "\n",
    "Function approximation is the process of approximating the value function using a function approximator, such as a neural network. The function approximator takes the state of the environment as input and outputs the expected reward of each action.\n",
    "$$\n",
    "Q(s,a) â‰ˆ Q(s,a;\\theta)\n",
    "$$\n",
    "\n",
    "where $Q(s,a;\\theta)$ is the estimated value of taking action a in state s using the parameters $\\theta$ of the function approximator.\n",
    "\n",
    "\n",
    "### Deep RL\n",
    "\n",
    "Deep RL is a type of Approximate RL that uses deep neural networks as function approximators. Deep RL algorithms have achieved impressive results in complex tasks, such as playing video games or controlling robots. The most popular Deep RL algorithm is the Deep Q-Network (DQN) algorithm.\n",
    "\n",
    "In summary, RL algorithms are a powerful tool for learning to make decisions in dynamic environments. Tabular RL works by constructing a table of expected rewards, while Approximate RL works by approximating the value function using a function approximator, such as a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e258c19",
   "metadata": {},
   "source": [
    "## Application on Simple Q-learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe618f",
   "metadata": {},
   "source": [
    "### FrozenLake Environment\n",
    "Let us create and train a simple Q-learning agent to solve the FrozenLake environment. \n",
    "The agent's policy is represented by the Q-table, which is updated over many episodes of interaction with the environment. Each entry in the Q-table represents the expected future reward for taking a particular action in a particular state, under the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "137999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "Q = {}\n",
    "\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 5000\n",
    "\n",
    "# List to contain total rewards per episode\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc0ac728",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "    s_tuple = env.reset()\n",
    "    s = str(s_tuple)  \n",
    "    Q.setdefault(s, np.zeros(env.action_space.n)) \n",
    "    done = False\n",
    "    j = 0\n",
    "    \n",
    "    rAll = 0\n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        a = np.argmax(Q[s] + np.random.randn(1, env.action_space.n)*(1./(i+1)))\n",
    "        step_info = env.step(a)\n",
    "        s1_tuple, reward, done, _, info = step_info  \n",
    "        s1 = str(s1_tuple) \n",
    "        Q.setdefault(s1, np.zeros(env.action_space.n))  \n",
    "        Q[s][a] = Q[s][a] + lr*(reward + y*np.max(Q[s1]) - Q[s][a])\n",
    "        rAll += reward\n",
    "        s = s1\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "219c04b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (0, {'prob': 1}), Actions: [0.         0.24330667 0.00708429 0.00746256]\n",
      "State: 1, Actions: [0.00134478 0.00223357 0.00129314 0.35044788]\n",
      "State: 0, Actions: [0.21857572 0.00605714 0.00977469 0.00765398]\n",
      "State: 4, Actions: [4.89329224e-01 1.70559008e-04 1.45848318e-03 1.36497386e-03]\n",
      "State: 8, Actions: [4.24658994e-03 4.05123593e-06 9.64118667e-03 2.66824480e-01]\n",
      "State: 9, Actions: [0.         0.6418312  0.00416471 0.        ]\n",
      "State: 13, Actions: [0.00831679 0.00329493 0.86972162 0.        ]\n",
      "State: 12, Actions: [0. 0. 0. 0.]\n",
      "State: 2, Actions: [0.00156657 0.00076447 0.00079863 0.13020447]\n",
      "State: 6, Actions: [2.76849154e-02 5.38305317e-07 3.33135853e-04 2.94084066e-05]\n",
      "State: 7, Actions: [0. 0. 0. 0.]\n",
      "State: 5, Actions: [0. 0. 0. 0.]\n",
      "State: 10, Actions: [7.32563049e-01 0.00000000e+00 6.38574181e-05 1.28010594e-04]\n",
      "State: 11, Actions: [0. 0. 0. 0.]\n",
      "State: 14, Actions: [0.         0.         0.98821046 0.        ]\n",
      "State: 3, Actions: [0.00014622 0.00037126 0.00069114 0.07308485]\n",
      "State: 15, Actions: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for state in Q:\n",
    "    print(f\"State: {state}, Actions: {Q[state]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be731fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 10 test games: 0.9\n"
     ]
    }
   ],
   "source": [
    "num_tests = 10\n",
    "total_reward = 0\n",
    "for i in range(num_tests):\n",
    "    s = str(env.reset())\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(Q[s])  # Choose the best action from Q table\n",
    "        s, reward, done, _, _ = env.step(a)\n",
    "        s = str(s)\n",
    "        total_reward += reward\n",
    "\n",
    "print(f\"Average reward over {num_tests} test games: {total_reward / num_tests}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45616328",
   "metadata": {},
   "source": [
    "The Q-Learning algorithm has learned action values for different states in the FrozenLake environment. The agent is not consistently reaching the goal state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
